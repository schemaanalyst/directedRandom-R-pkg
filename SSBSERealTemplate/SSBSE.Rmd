---
title: "My title"
#subtitle: "My subtitle"
author: Firstname Lastname
email: optional@email.com
institute: My optional Institute
university: My optional University
keywords: "Lorem, Ipsum"
abstract: "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua."
bibliography: references.bib
csl: lncs.csl
output:
  pdf_document:
    fig_caption: yes
    keep_tex: true
    template: lncs-template.tex
    #md_extensions: +footnotes
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(dplyr)
library(pander)
setwd("..")
knitr::opts_chunk$set(fig.pos = 'H')
mutationanalysistiming <- directedRandomR::collect_mutationanalysistime()
mutanttiming <- directedRandomR::collect_mutanttiming()
effect_size_thresholding <- directedRandomR::analyse_vargha_delaney_effect_size_thresholding(mutationanalysistiming)
wilcox_ranking <- directedRandomR::analyse_wilcox_rank_sum_test(mutationanalysistiming)
setwd("SSBSETemplate")
```

# Story Line

* Test Time Generation - Directed Random is faster in regards to AVM and AVM-Defaults.
* Coverage - As Directed Random is faster it has the same coverage as the rest of the techniques.
* Mutation Score - Directed Random has the higher or similar mutation score comparing to AVM and AVM-Defaults.


# RQs

## Test Time Generation

How efficent Directed Random in regard of test time generation comparing to the other data generators?

## Coverage

How do the number of test requirements generated by the coverage criteria differ depending on the data generation technique being used, and how successfully can test cases be automatically generated to satisfy them?

## Fault-Finding Effectiveness
How does fault finding effectiveness vary depending on data generation technique used?

# What Plots ?

## Test time generation (Effiency)

### Analyse Wilcox Rank and Effect Size for AVM and Directed Random

To  statistically  analyze  the  the new technique we conducted tests for significance with the nonparametric Wilcoxon rank-sum test, using the sets of 30 execution times obtained with a specific DBMS and all techniques **Hitchhicker Guide Ref**. A p-value that less than $0.05$ is considered significant. To review practical are significance tests we use the nonparametric A12 statistic of Vargha and Delaney **REF** was used to calculate effect sizes. The A12 determine the average probability that one approach beats another, or how superior one technique compared to the other.  We followed the guidelines of Vargha and Delaney in that an effect size is considered to be “large” if the value of A12 is $ < 0:29$ or $> 0.71$, “medium” if A12 is $< 0.36$ or $> 0.64$ and “small” if A12 is $< 0.44$ or $> 0.56$. However, is the values of A12 close to the 0.5 value are viewed as there no effect.

When comparing AVM and Directed Random techniques in regard of time we used Mann-Whitney U-test and the A-hat effect size calculations. As Shown in in the following two tables that there is statistically significat differance between Driected Random and AVM, $p \leq 0.05$. Therefore, we reject the null hypothesis that there is no difference between AVM and Directed Random. As p-value near zero, that directed random is faster than AVM in a statistically significant test. Moreover, the A-12 shows that Directed Random has a large effect size when it comes to test generation timing. Which menas that Directed Random is the winner in regard of test generation timing.


```{r wilcox_ranking, echo=FALSE}
#wilcox_ranking2 <- dplyr::select(wilcox_ranking, p.value, dbms)
wilcox_ranking2 <- wilcox_ranking %>% dplyr::filter(vs == "AVM vs Directed Random Test Generation Time") %>% dplyr::select(p.value, dbms)
knitr::kable(wilcox_ranking2, format = "latex")
```


Figure 1 and 2.

![Test generation timing - in seconds. First summing test generation timing for each run then group by data generator and DBMS, then divide test generation timing by 1000 to convert to second.](../plots/figure3.pdf)

![Box plot for Test Generation time for DBMS, techniques and schemas - in precentage. Group by data generator, case study and DBMS, then dividing test generation timning by 1000 to convert to second. No averaging or summing to see the spread of values for all runs per schema](../plots/figure4.pdf)

## Coverage

Figure 3.

![Box plot for Coverage](../plots/figure22.pdf)

## Mutation Score

Figure 4, 5 and 6.

![Box plot for mutation scores for DBMSs and techniques - in precentage. Frist I sum all score numerator and denominator per run per DBMS per data generator, then plottig using group by data generator, DBMS and divding the score numerator by denominator multiplying by 100](../plots/figure7.pdf)


![Box plot for mutation scores for DBMS, techniques and schemas - in precentage. Group by data generator, case study and DBMS, then divding the score numerator by denominator multiplying by 100. No averaging or summing to see the spread of values for all runs per schema](../plots/figure8.pdf)

![Box Plot of mutant scores in regard of Mutant Operators and DBMSs - in precentage. Using mutanttiming file, I group by generator, DBMS, schema and operator. Then I only select NORMAL mutants, then calculating the percentage of killed mutant by summing all killed divided by (killed mutant plus alive mutant) multiply by 100.](../plots/figure10.pdf)

